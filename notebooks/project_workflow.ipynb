{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "50a3d7eb",
      "metadata": {},
      "source": [
        "# Project 2: Customer Churn Prediction — Build Notebook\n",
        "\n",
        "This workbook mirrors the exploratory script before the project was modularized. Each section pastes the working code from `src/` so the entire workflow can be run sequentially inside the notebook. Streamlit app (`app.py`) is intentionally excluded.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d56cf08",
      "metadata": {},
      "source": [
        "## Step 0 — Environment Imports & Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbabbf14",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7b0343",
      "metadata": {},
      "source": [
        "## Step 1 — Data Preparation Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64663c81",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- data_prep.py ---\n",
        "# src/data_prep.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import textwrap\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DATA_URL = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\n",
        "PROCESSED_DIR = \"data/processed\"\n",
        "\n",
        "# ---- Helpers: teaching notes in comments ----\n",
        "def load_raw_data(url: str = DATA_URL) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load the IBM Telco Customer Churn dataset directly from the public URL.\n",
        "    Why direct URL? Keeps repo light and reproducible.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(url)\n",
        "    return df\n",
        "\n",
        "def clean_total_charges(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    The dataset sometimes has blank strings in TotalCharges (often when tenure==0).\n",
        "    Approach (explain & justify):\n",
        "      1) Coerce to numeric; blanks -> NaN\n",
        "      2) If tenure == 0 and TotalCharges is NaN: set to 0 (no months billed yet)\n",
        "      3) Else if tenure > 0 and TotalCharges is NaN: approximate as MonthlyCharges * tenure\n",
        "         (reasonable imputation tied to business meaning)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "\n",
        "    mask_tenure0 = (df[\"tenure\"] == 0) & (df[\"TotalCharges\"].isna())\n",
        "    df.loc[mask_tenure0, \"TotalCharges\"] = 0.0\n",
        "\n",
        "    mask_tenure_pos = (df[\"tenure\"] > 0) & (df[\"TotalCharges\"].isna())\n",
        "    df.loc[mask_tenure_pos, \"TotalCharges\"] = df.loc[mask_tenure_pos, \"MonthlyCharges\"] * df.loc[mask_tenure_pos, \"tenure\"]\n",
        "\n",
        "    # safety: still any NaNs? fill with median as a last resort\n",
        "    if df[\"TotalCharges\"].isna().any():\n",
        "        df[\"TotalCharges\"] = df[\"TotalCharges\"].fillna(df[\"TotalCharges\"].median())\n",
        "\n",
        "    return df\n",
        "\n",
        "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create simple, explainable features requested by the brief:\n",
        "      - tenure_bucket\n",
        "      - services_count\n",
        "      - monthly_to_total_ratio\n",
        "      - internet_no_tech_support (flag)\n",
        "      - Additional interaction/business signals to improve recall/precision\n",
        "      - Map target y: Churn {No,Yes} -> {0,1}\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # tenure_bucket\n",
        "    bins = [-0.1, 6, 12, 24, np.inf]\n",
        "    labels = [\"0-6m\", \"6-12m\", \"12-24m\", \"24m+\"]\n",
        "    df[\"tenure_bucket\"] = pd.cut(df[\"tenure\"], bins=bins, labels=labels)\n",
        "\n",
        "    # services_count: count how many services are \"on\"\n",
        "    # We'll treat 'Yes' as 1, and for InternetService (DSL/Fiber optic != 'No') as 1.\n",
        "    service_cols_yesno = [\n",
        "        \"PhoneService\", \"MultipleLines\", \"OnlineSecurity\", \"OnlineBackup\",\n",
        "        \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"\n",
        "    ]\n",
        "    services_yes = df[service_cols_yesno].apply(\n",
        "        lambda col: col.astype(str).str.strip().str.lower().eq(\"yes\")\n",
        "    )\n",
        "    count_yes = services_yes.astype(int).sum(axis=1)\n",
        "    internet_on = (df[\"InternetService\"].str.lower() != \"no\").astype(int)\n",
        "    df[\"services_count\"] = count_yes + internet_on\n",
        "\n",
        "    # monthly_to_total_ratio\n",
        "    denom = (df[\"tenure\"] * df[\"MonthlyCharges\"]).replace(0, 1)  # avoid divide-by-zero\n",
        "    df[\"monthly_to_total_ratio\"] = df[\"TotalCharges\"] / denom\n",
        "\n",
        "    # flag: internet but no tech support\n",
        "    df[\"internet_no_tech_support\"] = (\n",
        "        (df[\"InternetService\"].str.lower() != \"no\") &\n",
        "        (df[\"TechSupport\"].str.lower() == \"no\")\n",
        "    ).astype(int)\n",
        "\n",
        "    # --- Additional engineered features to help the models ---\n",
        "    # Ordinal encoding of tenure bucket for linear models\n",
        "    tenure_ord = {\"0-6m\": 0, \"6-12m\": 1, \"12-24m\": 2, \"24m+\": 3}\n",
        "    df[\"tenure_bucket_ord\"] = df[\"tenure_bucket\"].map(tenure_ord).fillna(0).astype(int)\n",
        "\n",
        "    # Auto-pay indicator\n",
        "    autopay_methods = {\"bank transfer (automatic)\", \"credit card (automatic)\"}\n",
        "    df[\"is_auto_pay\"] = df[\"PaymentMethod\"].str.strip().str.lower().isin(autopay_methods).astype(int)\n",
        "\n",
        "    # Long contract indicator (one or two year contracts)\n",
        "    df[\"is_long_contract\"] = df[\"Contract\"].str.contains(\"year\", case=False, na=False).astype(int)\n",
        "\n",
        "    # Streaming / support service counts\n",
        "    streaming_cols = [\"StreamingTV\", \"StreamingMovies\"]\n",
        "    support_cols = [\"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\"]\n",
        "    df[\"streaming_services\"] = df[streaming_cols].apply(\n",
        "        lambda col: col.astype(str).str.strip().str.lower().eq(\"yes\")\n",
        "    ).astype(int).sum(axis=1)\n",
        "    df[\"support_services\"] = df[support_cols].apply(\n",
        "        lambda col: col.astype(str).str.strip().str.lower().eq(\"yes\")\n",
        "    ).astype(int).sum(axis=1)\n",
        "\n",
        "    # Senior citizens on fiber optic internet (known high-risk segment)\n",
        "    df[\"senior_fiber_optic\"] = (\n",
        "        (df[\"SeniorCitizen\"] == 1) &\n",
        "        (df[\"InternetService\"].str.strip().str.lower() == \"fiber optic\")\n",
        "    ).astype(int)\n",
        "\n",
        "    # Charges per tenure month (helps capture early high spenders)\n",
        "    df[\"charges_per_month_of_tenure\"] = df[\"TotalCharges\"] / df[\"tenure\"].replace(0, 1)\n",
        "\n",
        "    # Target mapping\n",
        "    df[\"ChurnFlag\"] = (df[\"Churn\"].str.strip().str.lower() == \"yes\").astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def compute_expected_tenure_and_clv(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Baseline ExpectedTenure assumption (documented):\n",
        "      Month-to-month -> 6 months\n",
        "      One year      -> 12 months\n",
        "      Two year      -> 24 months\n",
        "    Then CLV = MonthlyCharges * ExpectedTenure\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    mapping = {\n",
        "        \"month-to-month\": 6,\n",
        "        \"one year\": 12,\n",
        "        \"two year\": 24,\n",
        "    }\n",
        "    df[\"ExpectedTenure\"] = df[\"Contract\"].str.strip().str.lower().map(mapping)\n",
        "    # fallback if any unexpected labels\n",
        "    df[\"ExpectedTenure\"] = df[\"ExpectedTenure\"].fillna(6)\n",
        "    df[\"CLV\"] = df[\"MonthlyCharges\"] * df[\"ExpectedTenure\"]\n",
        "    # Indicator for high monthly charges relative to CLV (above median ratio)\n",
        "    clv_ratio = df[\"MonthlyCharges\"] / df[\"CLV\"].replace(0, 1)\n",
        "    median_ratio = clv_ratio.median()\n",
        "    df[\"high_charge_to_clv_ratio\"] = (clv_ratio >= median_ratio).astype(int)\n",
        "    return df\n",
        "\n",
        "def select_model_columns(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Separate features and target for later modeling.\n",
        "    We keep raw categoricals for now; we'll one-hot encode in the model pipeline later.\n",
        "    \"\"\"\n",
        "    target = \"ChurnFlag\"\n",
        "    # keep core original columns + engineered ones:\n",
        "    feature_cols = [\n",
        "        # numerics\n",
        "        \"tenure\", \"MonthlyCharges\", \"TotalCharges\", \"CLV\", \"services_count\",\n",
        "        \"monthly_to_total_ratio\", \"internet_no_tech_support\",\n",
        "        \"tenure_bucket_ord\", \"is_auto_pay\", \"is_long_contract\",\n",
        "        \"streaming_services\", \"support_services\", \"senior_fiber_optic\",\n",
        "        \"charges_per_month_of_tenure\", \"high_charge_to_clv_ratio\",\n",
        "        # categoricals (kept as strings for now; encoder later)\n",
        "        \"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\",\n",
        "        \"PhoneService\", \"MultipleLines\", \"InternetService\",\n",
        "        \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\",\n",
        "        \"TechSupport\", \"StreamingTV\", \"StreamingMovies\",\n",
        "        \"Contract\", \"PaperlessBilling\", \"PaymentMethod\",\n",
        "        \"tenure_bucket\",\n",
        "    ]\n",
        "    # prune any missing columns defensively\n",
        "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
        "    X = df[feature_cols].copy()\n",
        "    y = df[target].copy()\n",
        "    return X, y\n",
        "\n",
        "def stratified_splits_and_save(X: pd.DataFrame, y: pd.Series, out_dir: str = PROCESSED_DIR, seed: int = 42):\n",
        "    \"\"\"\n",
        "    60/20/20 stratified split. Save CSVs to data/processed/.\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=0.4, random_state=seed, stratify=y\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, random_state=seed, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    def save_pair(Xd, yd, name):\n",
        "        df_out = Xd.copy()\n",
        "        df_out[\"ChurnFlag\"] = yd.values\n",
        "        df_out.to_csv(os.path.join(out_dir, f\"{name}.csv\"), index=False)\n",
        "\n",
        "    save_pair(X_train, y_train, \"train\")\n",
        "    save_pair(X_val, y_val, \"val\")\n",
        "    save_pair(X_test, y_test, \"test\")\n",
        "\n",
        "    print(f\"Saved: {os.path.join(out_dir,'train.csv')}\")\n",
        "    print(f\"       {os.path.join(out_dir,'val.csv')}\")\n",
        "    print(f\"       {os.path.join(out_dir,'test.csv')}\")\n",
        "\n",
        "def main():\n",
        "    print(\"Loading raw data…\")\n",
        "    df = load_raw_data()\n",
        "    print(f\"Rows: {len(df):,}\")\n",
        "\n",
        "    print(\"Cleaning TotalCharges…\")\n",
        "    df = clean_total_charges(df)\n",
        "\n",
        "    print(\"Engineering features…\")\n",
        "    df = engineer_features(df)\n",
        "\n",
        "    print(\"Computing ExpectedTenure + CLV…\")\n",
        "    df = compute_expected_tenure_and_clv(df)\n",
        "\n",
        "    print(\"Preparing splits…\")\n",
        "    X, y = select_model_columns(df)\n",
        "    stratified_splits_and_save(X, y)\n",
        "\n",
        "    print(textwrap.dedent(\"\"\"\n",
        "    ✅ Data prep complete.\n",
        "       - 60/20/20 splits saved to data/processed/\n",
        "       - Features include: tenure_bucket, services_count, monthly_to_total_ratio, internet_no_tech_support, CLV\n",
        "       Next: CLV quartiles + churn rate by quartile + charts.\n",
        "    \"\"\"))\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0438e5ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_prep_main = main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6537710e",
      "metadata": {},
      "source": [
        "## Step 2 — CLV Analysis Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495ac6c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- clv_analysis.py ---\n",
        "# src/clv_analysis.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PROCESSED_DIR = \"data/processed\"\n",
        "FIG_DIR = \"figures\"\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: load the processed splits\n",
        "# -----------------------------\n",
        "def load_splits(processed_dir: str = PROCESSED_DIR):\n",
        "    \"\"\"\n",
        "    We analyze on the TRAIN split to avoid peeking at validation/test.\n",
        "    Why? It prevents subtle information leakage in your narrative & prevents 'tuning' to test.\n",
        "    \"\"\"\n",
        "    train = pd.read_csv(os.path.join(processed_dir, \"train.csv\"))\n",
        "    val = pd.read_csv(os.path.join(processed_dir, \"val.csv\"))\n",
        "    test = pd.read_csv(os.path.join(processed_dir, \"test.csv\"))\n",
        "    return train, val, test\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: cut CLV into quartiles\n",
        "# -----------------------------\n",
        "def add_clv_quartile(df: pd.DataFrame, clv_col: str = \"CLV\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    pd.qcut makes equal-sized bins by rank (25% each).\n",
        "    We label them in ascending order of CLV.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    # qcut will fail if there are duplicate edges; 'duplicates=drop' handles ties gracefully.\n",
        "    out[\"CLV_quartile\"] = pd.qcut(out[clv_col], q=4, labels=[\"Low\", \"Medium\", \"High\", \"Premium\"], duplicates=\"drop\")\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: churn rate by quartile\n",
        "# -----------------------------\n",
        "def churn_rate_by_quartile(df: pd.DataFrame, y_col: str = \"ChurnFlag\"):\n",
        "    \"\"\"\n",
        "    Returns a small summary table with size and churn rate for each quartile.\n",
        "    churn_rate = mean of ChurnFlag (since it's 0/1).\n",
        "    \"\"\"\n",
        "    summary = (\n",
        "        df.groupby(\"CLV_quartile\")[y_col]\n",
        "          .agg(size=\"count\", churn_rate=\"mean\")\n",
        "          .reset_index()\n",
        "          .sort_values(\"CLV_quartile\")  # keeps the logical Low→Premium order\n",
        "    )\n",
        "    return summary\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: bar plot\n",
        "# -----------------------------\n",
        "def plot_churn_rate(summary: pd.DataFrame, savepath: str):\n",
        "    \"\"\"\n",
        "    Matplotlib only, single plot, no custom colors (keeps it clean & portable).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    # We multiply by 100 to show percentages\n",
        "    plt.bar(summary[\"CLV_quartile\"].astype(str), summary[\"churn_rate\"] * 100)\n",
        "    plt.title(\"Churn Rate by CLV Quartile\")\n",
        "    plt.xlabel(\"CLV Quartile\")\n",
        "    plt.ylabel(\"Churn Rate (%)\")\n",
        "    # Annotate bars with values\n",
        "    for x, y in zip(summary[\"CLV_quartile\"].astype(str), summary[\"churn_rate\"] * 100):\n",
        "        plt.text(x, y + 0.5, f\"{y:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(savepath, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: print narrative insights\n",
        "# -----------------------------\n",
        "def print_insights(summary: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Generates a few business-friendly bullets.\n",
        "    We compute relative differences to highlight strongest contrasts.\n",
        "    \"\"\"\n",
        "    # Convert to dicts for quick lookup\n",
        "    sdict = {row[\"CLV_quartile\"]: row for _, row in summary.iterrows()}\n",
        "    # Guard: if labels got collapsed (rare with duplicates=drop), handle gracefully\n",
        "    labels = [str(x) for x in summary[\"CLV_quartile\"].tolist()]\n",
        "\n",
        "    print(\"\\n===== Business Insights (auto-generated) =====\")\n",
        "    # 1) Overall ordering insight (Low vs Premium if both exist)\n",
        "    if \"Low\" in sdict and \"Premium\" in sdict:\n",
        "        low = sdict[\"Low\"][\"churn_rate\"] * 100\n",
        "        prem = sdict[\"Premium\"][\"churn_rate\"] * 100\n",
        "        diff = prem - low\n",
        "        trend = \"higher\" if diff > 0 else \"lower\"\n",
        "        print(f\"- Premium vs Low: Premium churn is {abs(diff):.1f} pp {trend} than Low \"\n",
        "              f\"({prem:.1f}% vs {low:.1f}%).\")\n",
        "    # 2) Highest-risk quartile\n",
        "    worst = summary.iloc[summary[\"churn_rate\"].argmax()]\n",
        "    print(f\"- Highest churn segment: {worst['CLV_quartile']} at {worst['churn_rate']*100:.1f}% churn.\")\n",
        "    # 3) Prioritization hint (top two)\n",
        "    top2 = summary.sort_values(\"churn_rate\", ascending=False).head(2)\n",
        "    segs = \", \".join([f\"{r['CLV_quartile']} ({r['churn_rate']*100:.1f}%)\" for _, r in top2.iterrows()])\n",
        "    print(f\"- Retention priority suggestion: focus on {segs}.\")\n",
        "\n",
        "    # 4) Size context\n",
        "    total = summary[\"size\"].sum()\n",
        "    for _, r in summary.iterrows():\n",
        "        pct = r[\"size\"] / total * 100 if total else 0\n",
        "        print(f\"  • {r['CLV_quartile']}: {r['size']} customers ({pct:.1f}% of train), churn {r['churn_rate']*100:.1f}%\")\n",
        "    print(\"=============================================\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n",
        "def main():\n",
        "    print(\"Loading processed splits…\")\n",
        "    train, val, test = load_splits()\n",
        "\n",
        "    # Basic sanity checks for required columns\n",
        "    needed = {\"CLV\", \"ChurnFlag\"}\n",
        "    missing = needed - set(train.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns in train split: {missing}. \"\n",
        "                         f\"Did you run src/data_prep.py successfully?\")\n",
        "\n",
        "    print(\"Adding CLV quartiles to TRAIN…\")\n",
        "    train_q = add_clv_quartile(train, clv_col=\"CLV\")\n",
        "\n",
        "    print(\"Computing churn rate by quartile…\")\n",
        "    summary = churn_rate_by_quartile(train_q, y_col=\"ChurnFlag\")\n",
        "    print(summary)\n",
        "\n",
        "    # Save a version with the quartile label (could be useful for the app)\n",
        "    out_csv = os.path.join(PROCESSED_DIR, \"train_with_clv_quartile.csv\")\n",
        "    train_q.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved: {out_csv}\")\n",
        "\n",
        "    # Plot & save\n",
        "    fig_path = os.path.join(FIG_DIR, \"churn_rate_by_clv_quartile.png\")\n",
        "    plot_churn_rate(summary, fig_path)\n",
        "    print(f\"Saved figure: {fig_path}\")\n",
        "\n",
        "    # Narrative insights\n",
        "    print_insights(summary)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f60b36",
      "metadata": {},
      "outputs": [],
      "source": [
        "clv_main = main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edaed942",
      "metadata": {},
      "source": [
        "## Step 3 — Model Training Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b72b11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- model_train.py ---\n",
        "# src/model_train.py\n",
        "# Train LogReg, RandomForest, and XGBoost with light tuning + imbalance care.\n",
        "# Select a recall-first operating threshold on validation, then freeze for test.\n",
        "# Saves:\n",
        "#   models/logreg_pipeline.pkl              (best model = usually LOGREG here)\n",
        "#   models/logreg_metrics.json              (chosen/best model metrics)\n",
        "#   models/metrics_all.json                 (per-model val/test)\n",
        "#   models/roc_curves_test.json             (for the ROC overlay in the app)\n",
        "\n",
        "import inspect\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    confusion_matrix,\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Optional: XGBoost (skip gracefully if not present)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    _HAS_XGB = True\n",
        "except Exception as e:\n",
        "    _HAS_XGB = False\n",
        "    _XGB_ERR = str(e)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "MODELS_DIR = \"models\"\n",
        "DATA_DIR = \"data/processed\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "VAL_RECALL_MIN = 0.60  # recall-first policy on validation\n",
        "THRESH_GRID = np.round(np.linspace(0.30, 0.80, 51), 3)  # 0.30 → 0.80 (step ~0.01)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "\n",
        "# ------------------------- Data Loading -------------------------\n",
        "def _load_splits() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    paths = [os.path.join(DATA_DIR, f) for f in [\"train.csv\", \"val.csv\", \"test.csv\"]]\n",
        "    if not all(os.path.exists(p) for p in paths):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Processed splits not found in {DATA_DIR}. Run: python -m src.data_prep\"\n",
        "        )\n",
        "    train = pd.read_csv(paths[0])\n",
        "    val   = pd.read_csv(paths[1])\n",
        "    test  = pd.read_csv(paths[2])\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "# ------------------------- Features -------------------------\n",
        "def _feature_lists(df: pd.DataFrame) -> Tuple[list, list]:\n",
        "    # Numeric engineered features you created in data_prep.py\n",
        "    num_cols = [\n",
        "        \"tenure\",\n",
        "        \"MonthlyCharges\",\n",
        "        \"TotalCharges\",\n",
        "        \"CLV\",\n",
        "        \"services_count\",\n",
        "        \"monthly_to_total_ratio\",\n",
        "        \"internet_no_tech_support\",\n",
        "        \"tenure_bucket_ord\",\n",
        "        \"is_auto_pay\",\n",
        "        \"is_long_contract\",\n",
        "        \"streaming_services\",\n",
        "        \"support_services\",\n",
        "        \"senior_fiber_optic\",\n",
        "        \"charges_per_month_of_tenure\",\n",
        "        \"high_charge_to_clv_ratio\",\n",
        "    ]\n",
        "    # Categorical features\n",
        "    cat_cols = [\n",
        "        \"gender\",\n",
        "        \"SeniorCitizen\",           # treat as categorical (0/1)\n",
        "        \"Partner\",\n",
        "        \"Dependents\",\n",
        "        \"PhoneService\",\n",
        "        \"MultipleLines\",\n",
        "        \"InternetService\",\n",
        "        \"OnlineSecurity\",\n",
        "        \"OnlineBackup\",\n",
        "        \"DeviceProtection\",\n",
        "        \"TechSupport\",\n",
        "        \"StreamingTV\",\n",
        "        \"StreamingMovies\",\n",
        "        \"Contract\",\n",
        "        \"PaperlessBilling\",\n",
        "        \"PaymentMethod\",\n",
        "        \"tenure_bucket\",\n",
        "    ]\n",
        "    # Keep only columns that actually exist\n",
        "    num_cols = [c for c in num_cols if c in df.columns]\n",
        "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
        "    return num_cols, cat_cols\n",
        "\n",
        "\n",
        "def _preprocessor(num_cols: list, cat_cols: list) -> ColumnTransformer:\n",
        "    # scikit-learn >=1.4 renamed `sparse` to `sparse_output`; handle both.\n",
        "    ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n",
        "    if \"sparse_output\" in OneHotEncoder.__init__.__code__.co_varnames:\n",
        "        ohe_kwargs[\"sparse_output\"] = True\n",
        "    else:\n",
        "        ohe_kwargs[\"sparse\"] = True\n",
        "    cat_encoder = OneHotEncoder(**ohe_kwargs)\n",
        "\n",
        "    num_pipe = Pipeline([(\"scaler\", StandardScaler())])\n",
        "    cat_pipe = Pipeline([(\"ohe\", cat_encoder)])\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", num_pipe, num_cols),\n",
        "            (\"cat\", cat_pipe, cat_cols),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3,\n",
        "    )\n",
        "    return pre\n",
        "\n",
        "\n",
        "# ------------------------- Models -------------------------\n",
        "def make_logreg() -> LogisticRegression:\n",
        "    # class_weight balances the minority (churn) class\n",
        "    return LogisticRegression(\n",
        "        penalty=\"l2\",\n",
        "        C=1.0,\n",
        "        solver=\"lbfgs\",\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\",\n",
        "        n_jobs=None,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "\n",
        "def make_rf() -> RandomForestClassifier:\n",
        "    # Light tuning that generalizes well on Telco; balanced_subsample helps imbalance\n",
        "    return RandomForestClassifier(\n",
        "        n_estimators=600,\n",
        "        max_depth=10,\n",
        "        min_samples_leaf=20,\n",
        "        class_weight=\"balanced_subsample\",\n",
        "        n_jobs=-1,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "\n",
        "def make_xgb(scale_pos_weight: float):\n",
        "    # Common, robust defaults + early stopping (handled in fit block)\n",
        "    return XGBClassifier(\n",
        "        n_estimators=800,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        min_child_weight=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_lambda=2.0,\n",
        "        gamma=0.0,\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"auc\",\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        random_state=RANDOM_STATE,\n",
        "        tree_method=\"hist\",\n",
        "        n_jobs=-1,\n",
        "        verbosity=0,\n",
        "    )\n",
        "\n",
        "\n",
        "# ------------------------- Metrics & Thresholds -------------------------\n",
        "@dataclass\n",
        "class EvalResult:\n",
        "    metrics_val: Dict\n",
        "    metrics_test: Dict\n",
        "    thr: float\n",
        "    roc_test: Dict[str, list]\n",
        "\n",
        "\n",
        "def _bin_metrics(y_true, y_prob, thr) -> Dict:\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"binary\", zero_division=0\n",
        "    )\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return {\n",
        "        \"precision\": float(p),\n",
        "        \"recall\": float(r),\n",
        "        \"f1\": float(f1),\n",
        "        \"auc\": float(auc),\n",
        "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
        "        \"used_threshold\": float(thr),\n",
        "    }\n",
        "\n",
        "\n",
        "def _choose_threshold_recall_first(y_true, y_prob, recall_min=VAL_RECALL_MIN):\n",
        "    best_thr = 0.50\n",
        "    best_f1 = -1.0\n",
        "    met = None\n",
        "    for thr in THRESH_GRID:\n",
        "        m = _bin_metrics(y_true, y_prob, thr)\n",
        "        if m[\"recall\"] >= recall_min and m[\"f1\"] > best_f1:\n",
        "            best_f1 = m[\"f1\"]\n",
        "            best_thr = thr\n",
        "            met = m\n",
        "    if met is None:\n",
        "        # If no threshold reaches target recall, fall back to best f1 overall\n",
        "        for thr in THRESH_GRID:\n",
        "            m = _bin_metrics(y_true, y_prob, thr)\n",
        "            if m[\"f1\"] > best_f1:\n",
        "                best_f1 = m[\"f1\"]; best_thr = thr; met = m\n",
        "    return best_thr, met\n",
        "\n",
        "\n",
        "def _roc_points(y_true, y_prob) -> Tuple[list, list]:\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    return list(map(float, fpr)), list(map(float, tpr))\n",
        "\n",
        "\n",
        "# ------------------------- Train/Eval -------------------------\n",
        "def train_and_eval():\n",
        "    train, val, test = _load_splits()\n",
        "    y_tr = train[\"ChurnFlag\"].astype(int).values\n",
        "    y_va = val[\"ChurnFlag\"].astype(int).values\n",
        "    y_te = test[\"ChurnFlag\"].astype(int).values\n",
        "\n",
        "    X_tr = train.drop(columns=[\"ChurnFlag\"])\n",
        "    X_va = val.drop(columns=[\"ChurnFlag\"])\n",
        "    X_te = test.drop(columns=[\"ChurnFlag\"])\n",
        "\n",
        "    num_cols, cat_cols = _feature_lists(train)\n",
        "    pre = _preprocessor(num_cols, cat_cols)\n",
        "\n",
        "    # Pos/neg counts for XGB scale_pos_weight\n",
        "    pos = int(y_tr.sum())\n",
        "    neg = int((y_tr == 0).sum())\n",
        "    spw = neg / max(1, pos)\n",
        "\n",
        "    results = {}\n",
        "    roc_dict = {}\n",
        "\n",
        "    # ---------- LOGREG ----------\n",
        "    logreg = make_logreg()\n",
        "    pipe_logreg = Pipeline(steps=[(\"pre\", pre), (\"model\", logreg)])\n",
        "    pipe_logreg.fit(X_tr, y_tr)\n",
        "    joblib.dump(pipe_logreg, os.path.join(MODELS_DIR, \"logreg_baseline_pipeline.pkl\"))\n",
        "\n",
        "    prob_va = pipe_logreg.predict_proba(X_va)[:, 1]\n",
        "    thr, met_val = _choose_threshold_recall_first(y_va, prob_va, VAL_RECALL_MIN)\n",
        "    prob_te = pipe_logreg.predict_proba(X_te)[:, 1]\n",
        "    met_test = _bin_metrics(y_te, prob_te, thr)\n",
        "    fpr, tpr = _roc_points(y_te, prob_te)\n",
        "    roc_dict[\"logreg\"] = {\"fpr\": fpr, \"tpr\": tpr}\n",
        "\n",
        "    results[\"logreg\"] = {\"val\": met_val, \"test\": met_test, \"thr\": thr}\n",
        "\n",
        "    # ---------- RANDOM FOREST ----------\n",
        "    rf = make_rf()\n",
        "    pipe_rf = Pipeline(steps=[(\"pre\", pre), (\"model\", rf)])\n",
        "    pipe_rf.fit(X_tr, y_tr)\n",
        "\n",
        "    prob_va = pipe_rf.predict_proba(X_va)[:, 1]\n",
        "    thr_rf, met_val_rf = _choose_threshold_recall_first(y_va, prob_va, VAL_RECALL_MIN)\n",
        "    prob_te = pipe_rf.predict_proba(X_te)[:, 1]\n",
        "    met_test_rf = _bin_metrics(y_te, prob_te, thr_rf)\n",
        "    fpr, tpr = _roc_points(y_te, prob_te)\n",
        "    roc_dict[\"rf\"] = {\"fpr\": fpr, \"tpr\": tpr}\n",
        "    results[\"rf\"] = {\"val\": met_val_rf, \"test\": met_test_rf, \"thr\": thr_rf}\n",
        "    joblib.dump(pipe_rf, os.path.join(MODELS_DIR, \"rf_pipeline.pkl\"))\n",
        "\n",
        "    # ---------- XGBOOST (optional) ----------\n",
        "    if _HAS_XGB:\n",
        "        xgb = make_xgb(spw)\n",
        "        # Fit XGB with early stopping on validation\n",
        "        # We pass the preprocessed arrays explicitly to leverage early stopping.\n",
        "        X_tr_pre = pipe_logreg.named_steps[\"pre\"].fit_transform(X_tr)  # reuse pre to get feature matrix\n",
        "        X_va_pre = pipe_logreg.named_steps[\"pre\"].transform(X_va)\n",
        "\n",
        "        # XGBoost>=2.1 prefers callbacks for early stopping; fall back for older versions.\n",
        "        fit_kwargs = {\n",
        "            \"X\": X_tr_pre,\n",
        "            \"y\": y_tr,\n",
        "            \"eval_set\": [(X_va_pre, y_va)],\n",
        "            \"verbose\": False,\n",
        "        }\n",
        "        fit_sig = inspect.signature(xgb.fit)\n",
        "        try:\n",
        "            if \"callbacks\" in fit_sig.parameters:\n",
        "                try:\n",
        "                    from xgboost.callback import EarlyStopping\n",
        "                    callbacks = [EarlyStopping(rounds=50, save_best=True)]\n",
        "                    xgb.fit(**fit_kwargs, callbacks=callbacks)\n",
        "                except Exception:\n",
        "                    xgb.fit(**fit_kwargs)\n",
        "            elif \"early_stopping_rounds\" in fit_sig.parameters:\n",
        "                xgb.fit(**fit_kwargs, early_stopping_rounds=50)\n",
        "            else:\n",
        "                xgb.fit(**fit_kwargs)\n",
        "        except TypeError:\n",
        "            xgb.fit(**fit_kwargs)\n",
        "        # Build a pipeline wrapper so app can use a consistent interface\n",
        "        pipe_xgb = Pipeline(steps=[(\"pre\", pre), (\"model\", xgb)])\n",
        "\n",
        "        prob_va = pipe_xgb.predict_proba(X_va)[:, 1]\n",
        "        thr_xgb, met_val_xgb = _choose_threshold_recall_first(y_va, prob_va, VAL_RECALL_MIN)\n",
        "        prob_te = pipe_xgb.predict_proba(X_te)[:, 1]\n",
        "        met_test_xgb = _bin_metrics(y_te, prob_te, thr_xgb)\n",
        "        fpr, tpr = _roc_points(y_te, prob_te)\n",
        "        roc_dict[\"xgb\"] = {\"fpr\": fpr, \"tpr\": tpr}\n",
        "        results[\"xgb\"] = {\"val\": met_val_xgb, \"test\": met_test_xgb, \"thr\": thr_xgb}\n",
        "        joblib.dump(pipe_xgb, os.path.join(MODELS_DIR, \"xgb_pipeline.pkl\"))\n",
        "    else:\n",
        "        print(f\"⚠️ XGBoost not available, skipping. Error: {_XGB_ERR}\")\n",
        "\n",
        "    # ---------------- Choose best model by validation F1 under recall constraint ----------------\n",
        "    # (You can change selection logic if the brief asks for a different tie-breaker.)\n",
        "    def key_fn(k):\n",
        "        return results[k][\"val\"][\"f1\"]\n",
        "\n",
        "    best_name = max(results.keys(), key=key_fn)\n",
        "    best_thr = results[best_name][\"thr\"]\n",
        "\n",
        "    # Save the *LogReg* pipeline as default (the app expects this path);\n",
        "    # if another model won, we still save the winning pipeline under this path for simplicity.\n",
        "    best_pipe = {\n",
        "        \"logreg\": pipe_logreg,\n",
        "        \"rf\": pipe_rf,\n",
        "        \"xgb\": pipe_xgb if _HAS_XGB else pipe_logreg,\n",
        "    }[best_name]\n",
        "\n",
        "    # Persist the winning pipeline + its metrics\n",
        "    joblib.dump(best_pipe, os.path.join(MODELS_DIR, \"logreg_pipeline.pkl\"))\n",
        "\n",
        "    with open(os.path.join(MODELS_DIR, \"logreg_metrics.json\"), \"w\") as f:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"val\": results[best_name][\"val\"],\n",
        "                \"test\": results[best_name][\"test\"],\n",
        "                \"used_threshold\": best_thr,\n",
        "                \"model_name\": best_name,\n",
        "            },\n",
        "            f,\n",
        "            indent=2,\n",
        "        )\n",
        "\n",
        "    # Persist per-model metrics for the app comparison table\n",
        "    with open(os.path.join(MODELS_DIR, \"metrics_all.json\"), \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Persist ROC curves for the app overlay\n",
        "    with open(os.path.join(MODELS_DIR, \"roc_curves_test.json\"), \"w\") as f:\n",
        "        json.dump(roc_dict, f, indent=2)\n",
        "\n",
        "    # Convenience prints\n",
        "    print(f\"\\n=== BEST MODEL: {best_name.upper()} (test at thr={best_thr:.2f}) ===\")\n",
        "    print(results[best_name][\"test\"])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b842e2eb",
      "metadata": {},
      "source": [
        "## Step 4 — Global Interpretability Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de95fc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- interpretability.py ---\n",
        "# src/interpretability.py\n",
        "import os, json, joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "FIG_DIR = Path(\"figures\"); FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR = Path(\"models\")\n",
        "DATA_DIR = Path(\"data/processed\")\n",
        "\n",
        "def _load_splits():\n",
        "    train = pd.read_csv(DATA_DIR/\"train.csv\")\n",
        "    val   = pd.read_csv(DATA_DIR/\"val.csv\")\n",
        "    test  = pd.read_csv(DATA_DIR/\"test.csv\")\n",
        "    return train, val, test\n",
        "\n",
        "def _safe_load(path):\n",
        "    return joblib.load(path) if path.exists() else None\n",
        "\n",
        "def logreg_importance():\n",
        "    pipe = _safe_load(MODELS_DIR/\"logreg_pipeline.pkl\")\n",
        "    if pipe is None:\n",
        "        print(\"LogReg pipeline missing; skip.\"); return\n",
        "    pre, model = pipe.named_steps[\"pre\"], pipe.named_steps[\"model\"]\n",
        "\n",
        "    # compute feature std on training preprocessed data (for standardized importances)\n",
        "    train, _, _ = _load_splits()\n",
        "    Xtr = train.drop(columns=[\"ChurnFlag\"])\n",
        "    Xtr_pre = pre.transform(Xtr)\n",
        "    Xtr_pre = Xtr_pre.toarray() if hasattr(Xtr_pre, \"toarray\") else np.asarray(Xtr_pre)\n",
        "    feat_names = pre.get_feature_names_out()\n",
        "\n",
        "    if hasattr(model, \"coef_\"):\n",
        "        std = Xtr_pre.std(axis=0)\n",
        "        coefs = model.coef_.ravel()\n",
        "        imp = np.abs(coefs * std)\n",
        "        title = \"LogReg global importance (|coef × std|)\"\n",
        "    elif hasattr(model, \"feature_importances_\"):\n",
        "        imp = model.feature_importances_\n",
        "        title = f\"{model.__class__.__name__} feature importances\"\n",
        "    else:\n",
        "        print(f\"{model.__class__.__name__} lacks coefficients/feature_importances; skipping global plot.\")\n",
        "        return\n",
        "\n",
        "    top_idx = np.argsort(imp)[::-1][:20]\n",
        "    top_feats = feat_names[top_idx]\n",
        "    top_vals  = imp[top_idx]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.barh(range(len(top_vals))[::-1], top_vals[::-1])\n",
        "    plt.yticks(range(len(top_vals))[::-1], top_feats[::-1])\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    out = FIG_DIR/\"logreg_global_importance.png\"\n",
        "    plt.savefig(out, dpi=200); plt.close()\n",
        "    print(f\"Saved {out}\")\n",
        "\n",
        "def shap_trees(which=\"rf\"):\n",
        "    try:\n",
        "        import shap\n",
        "    except Exception as e:\n",
        "        print(\"SHAP not available; skip trees.\", e); return\n",
        "    model_path = MODELS_DIR/(f\"{which}_pipeline.pkl\")\n",
        "    pipe = _safe_load(model_path)\n",
        "    if pipe is None:\n",
        "        print(f\"{which.upper()} pipeline missing; skip.\"); return\n",
        "\n",
        "    pre, model = pipe.named_steps[\"pre\"], pipe.named_steps[\"model\"]\n",
        "    _, _, test = _load_splits()\n",
        "    X = test.drop(columns=[\"ChurnFlag\"])\n",
        "    # sample to keep it snappy\n",
        "    Xs = X.sample(min(300, len(X)), random_state=42)\n",
        "    Xs_pre = pre.transform(Xs)\n",
        "    if hasattr(Xs_pre, \"toarray\"):\n",
        "        Xs_pre = Xs_pre.toarray()\n",
        "    else:\n",
        "        Xs_pre = np.asarray(Xs_pre)\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(Xs_pre)\n",
        "    if isinstance(shap_values, list) and len(shap_values) > 1:\n",
        "        shap_values_use = shap_values[-1]\n",
        "    else:\n",
        "        shap_values_use = shap_values\n",
        "    # Global bar plot\n",
        "    feat_names = pre.get_feature_names_out()\n",
        "    plt.figure()\n",
        "    shap.summary_plot(shap_values_use, features=Xs_pre, feature_names=feat_names, plot_type=\"bar\", show=False)\n",
        "    out = FIG_DIR/f\"{which}_shap_global_bar.png\"\n",
        "    plt.tight_layout(); plt.savefig(out, dpi=200); plt.close()\n",
        "    print(f\"Saved {out}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logreg_importance()\n",
        "    shap_trees(\"rf\")\n",
        "    shap_trees(\"xgb\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d210d40",
      "metadata": {},
      "source": [
        "## Step 5 — Local Interpretability Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eef4681",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- interpret.py ---\n",
        "# src/interpret.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import math\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List\n",
        "\n",
        "MODELS_DIR = \"models\"\n",
        "FIG_DIR = \"figures\"\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "def get_feature_names(preprocessor) -> List[str]:\n",
        "    \"\"\"\n",
        "    Recover the final feature names after ColumnTransformer + OneHotEncoder.\n",
        "    Works in sklearn>=1.0 where get_feature_names_out is supported.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        names = preprocessor.get_feature_names_out()\n",
        "        return names.tolist()\n",
        "    except Exception:\n",
        "        # Fallback: build names manually for older versions (unlikely with your env)\n",
        "        names = []\n",
        "        for name, trans, cols in preprocessor.transformers_:\n",
        "            if name == \"remainder\" and trans == \"drop\":\n",
        "                continue\n",
        "            if hasattr(trans, \"named_steps\") and \"ohe\" in trans.named_steps:\n",
        "                # categorical pipeline\n",
        "                ohe = trans.named_steps[\"ohe\"]\n",
        "                imputer = trans.named_steps.get(\"imputer\", None)\n",
        "                base_cols = cols\n",
        "                # Handle case where imputer may change dtype but not names\n",
        "                cat_names = ohe.get_feature_names_out(base_cols)\n",
        "                names.extend(cat_names.tolist())\n",
        "            else:\n",
        "                # numeric pipeline: names are just cols\n",
        "                names.extend(cols if isinstance(cols, list) else list(cols))\n",
        "        return names\n",
        "\n",
        "def main():\n",
        "    # Load best pipeline (we saved logreg as best)\n",
        "    model_path = os.path.join(MODELS_DIR, \"logreg_pipeline.pkl\")\n",
        "    pipe = joblib.load(model_path)\n",
        "\n",
        "    pre = pipe.named_steps[\"pre\"]\n",
        "    model = pipe.named_steps[\"model\"]\n",
        "\n",
        "    # We handle linear models via coefficients; otherwise fall back to feature_importances.\n",
        "    use_coeffs = hasattr(model, \"coef_\")\n",
        "    use_importance = hasattr(model, \"feature_importances_\")\n",
        "\n",
        "    if not use_coeffs and not use_importance:\n",
        "        raise ValueError(\n",
        "            f\"{model.__class__.__name__} lacks coefficients or feature_importances_. \"\n",
        "            \"Cannot generate interpretability artifacts.\"\n",
        "        )\n",
        "\n",
        "    # Get expanded feature names after preprocessing\n",
        "    feat_names = get_feature_names(pre)\n",
        "\n",
        "    # Coefficients (shape: [1, n_features])\n",
        "    if use_coeffs:\n",
        "        values = model.coef_.ravel()\n",
        "    else:\n",
        "        values = model.feature_importances_.ravel()\n",
        "    if len(values) != len(feat_names):\n",
        "        raise ValueError(f\"Mismatch: {len(values)} weights vs {len(feat_names)} feature names\")\n",
        "\n",
        "    # Build a tidy table\n",
        "    df_imp = pd.DataFrame({\n",
        "        \"feature\": feat_names,\n",
        "        \"value\": values,\n",
        "    })\n",
        "    if use_coeffs:\n",
        "        df_imp[\"odds_ratio\"] = np.exp(df_imp[\"value\"])\n",
        "    else:\n",
        "        df_imp[\"odds_ratio\"] = np.nan  # not meaningful for tree models\n",
        "    df_imp[\"abs_value\"] = df_imp[\"value\"].abs()\n",
        "\n",
        "    # Sort for top drivers\n",
        "    sort_col = \"value\"\n",
        "    top_positive = df_imp.sort_values(sort_col, ascending=False).head(20)\n",
        "    top_negative = df_imp.sort_values(sort_col, ascending=True).head(20)\n",
        "\n",
        "    # Save CSVs\n",
        "    out_csv_all = os.path.join(MODELS_DIR, \"logreg_feature_importance_full.csv\")\n",
        "    out_csv_pos = os.path.join(MODELS_DIR, \"logreg_top_positive.csv\")\n",
        "    out_csv_neg = os.path.join(MODELS_DIR, \"logreg_top_negative.csv\")\n",
        "    df_imp.to_csv(out_csv_all, index=False)\n",
        "    top_positive.to_csv(out_csv_pos, index=False)\n",
        "    top_negative.to_csv(out_csv_neg, index=False)\n",
        "\n",
        "    print(f\"Saved: {out_csv_all}\")\n",
        "    print(f\"Saved: {out_csv_pos}\")\n",
        "    print(f\"Saved: {out_csv_neg}\")\n",
        "\n",
        "    # --- PLOTS (horizontal bar charts) ---\n",
        "    def plot_barh(df, title, savepath):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        df_plot = df.copy()\n",
        "        df_plot = df_plot.sort_values(sort_col, ascending=True)\n",
        "        labels = df_plot[\"feature\"].str.replace(\"cat__ohe__\", \"\", regex=False)\n",
        "        plt.barh(labels, df_plot[sort_col])\n",
        "        plt.xlabel(\"Coefficient\" if use_coeffs else \"Feature Importance\")\n",
        "        plt.title(title)\n",
        "        if use_coeffs:\n",
        "            for y, (_, row) in enumerate(df_plot.iterrows()):\n",
        "                plt.text(row[sort_col] + 0.01, y, f\"{row['odds_ratio']:.2f}\", va=\"center\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(savepath, dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "    pos_title = \"Top Positive Drivers of Churn (higher → more likely to churn)\" if use_coeffs else \"Top Features Increasing Predicted Risk\"\n",
        "    neg_title = \"Top Negative Drivers of Churn (higher → less likely to churn)\" if use_coeffs else \"Top Features Decreasing Predicted Risk\"\n",
        "    plot_barh(top_positive, pos_title, os.path.join(FIG_DIR, \"logreg_top_positive_odds.png\"))\n",
        "    plot_barh(top_negative, neg_title, os.path.join(FIG_DIR, \"logreg_top_negative_odds.png\"))\n",
        "\n",
        "    print(\"Saved figures:\")\n",
        "    print(\" - figures/logreg_top_positive_odds.png\")\n",
        "    print(\" - figures/logreg_top_negative_odds.png\")\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37083e81",
      "metadata": {},
      "outputs": [],
      "source": [
        "interpret_main = main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bba5d2d1",
      "metadata": {},
      "source": [
        "### Execute Step 1 — Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe608c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "raw_df = load_raw_data()\n",
        "clean_df = clean_total_charges(raw_df)\n",
        "feature_df = engineer_features(clean_df)\n",
        "feature_df = compute_expected_tenure_and_clv(feature_df)\n",
        "X, y = select_model_columns(feature_df)\n",
        "stratified_splits_and_save(X, y)\n",
        "print(feature_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b22378e",
      "metadata": {},
      "source": [
        "### Execute Step 2 — CLV Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "152e3979",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_df = pd.read_csv(PROCESSED_DIR / \"train.csv\")\n",
        "val_df = pd.read_csv(PROCESSED_DIR / \"val.csv\")\n",
        "test_df = pd.read_csv(PROCESSED_DIR / \"test.csv\")\n",
        "\n",
        "train_q = add_clv_quartile(train_df)\n",
        "clv_summary = churn_rate_by_quartile(train_q)\n",
        "print(clv_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f244f01a",
      "metadata": {},
      "source": [
        "### Execute Step 3 — Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d279dc04",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_and_eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db704a1b",
      "metadata": {},
      "source": [
        "### Execute Step 4 — Global Interpretability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de561f04",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "logreg_importance()\n",
        "shap_trees('rf')\n",
        "shap_trees('xgb')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a62cfed",
      "metadata": {},
      "source": [
        "### Execute Step 5 — Local Interpretability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c8f7105",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "interpret_main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
