# Customer Churn Prediction & CLV

Predict which telecom customers are likely to churn, estimate their Customer Lifetime Value (CLV), and prioritize retention with a Streamlit dashboard backed by scikit-learn models.

**Live app:** <https://praise-enato-churn-project.streamlit.app/>  
**Repo clone URL:** <https://github.com/Praise-Enato/Data-Science-project2-churn-prediction.git>

---

## Project Highlights

- End-to-end churn modeling pipeline on the IBM Telco Customer Churn dataset with reproducible preprocessing and stratified splits.
- Logistic Regression as the primary model with Random Forest and XGBoost benchmarks, tuned to meet a recall >= 0.60 operating target.
- CLV estimation and quartile segmentation to align retention focus with customer value.
- Streamlit app with Predict, Model Performance, and CLV Overview tabs plus cached assets for fast responses.

## Repository Structure

.
├── app.py                  # Streamlit entry point
├── data/
│   ├── raw/                # optional raw dataset cache; pulled on demand
│   └── processed/          # train/val/test splits generated by data_prep.py
├── figures/                # saved plots for documentation and the app
├── models/                 # serialized pipelines, metrics, and ROC curves
├── notebooks/              # exploratory notebooks (not required to run)
├── requirements.txt
└── src/
    ├── data_prep.py        # download + clean + feature engineering + splits
    ├── clv_analysis.py     # CLV quartile summaries and figures
    ├── model_train.py      # train/evaluate Logistic Regression, RF, XGB
    ├── interpretability.py # global feature importance + ROC artifacts
    └── interpret.py        # local explanation helpers for the app

## Data & Feature Engineering

- Source dataset: [IBM Telco Customer Churn](https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv); downloaded automatically in `src/data_prep.py`.
- Cleans `TotalCharges`, imputes missing values, and engineers tenure buckets, service counts, risk flags, and CLV.
- Stratified 60/20/20 train, validation, and test splits are saved to `data/processed/`.

## Modeling & Evaluation

- Primary model: Logistic Regression (with one-hot encoding and scaling in a scikit-learn pipeline).
- Baselines: Random Forest and XGBoost for comparison.
- Operating policy: choose validation thresholds meeting recall >= 0.60 and report frozen metrics on the test set.
- Metrics and ROC curve JSON files live under `models/` for reuse in the Streamlit app.

## Streamlit App

- **Predict tab:** Collect customer attributes, return churn probability, risk badge, CLV estimate, and a coefficient-based local explanation.
- **Model Performance tab:** Threshold-aware precision, recall, F1, ROC-AUC, confusion matrix, ROC overlays, and optional calibration.
- **CLV Overview tab:** CLV distribution, churn by quartile, and tailored retention takeaways.
- Uses `@st.cache_data` and `@st.cache_resource` to keep load times under roughly two seconds on Streamlit Community Cloud.

## Getting Started

### 1. Set up a virtual environment

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 2. Install dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 3. Build local artifacts (only needed after data or code changes)

```bash
python -m src.data_prep         # downloads and preprocesses the dataset
python -m src.clv_analysis      # generates CLV summaries and plots
python -m src.model_train       # fits models and saves metrics and pipelines
python -m src.interpretability  # optional: refresh global interpretation plots
```

### 4. Launch the Streamlit app

```bash
streamlit run app.py
```

## Deployment Notes

- Targeted for Streamlit Community Cloud; push regenerated artifacts (`models/`, `figures/`, `data/processed/`) before deploying.
- Configure Streamlit secrets or environment variables as needed; none are required by default.

## Troubleshooting

- If `src.data_prep` raises an HTTP error, verify your network connection and rerun the command.
- Regenerate artifacts after modifying feature engineering or model code to keep the deployed app consistent.
